{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Sean Leong Wei Kok\n",
    "\n",
    "\n",
    "#https://studentplanner-1712e.firebaseio.com/\n",
    "\n",
    "import speech_recognition as sr\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import punctuator \n",
    "import os\n",
    "import pandas as pd \n",
    "import json \n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "from textblob import TextBlob\n",
    "\n",
    "import pyrebase\n",
    "\n",
    "\n",
    "import ast\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER_NAME = 'INPUT'\n",
    "PUNCTUATED_FOLDER_NAME = 'OUTPUT'\n",
    "\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "INPUT_FOLDER = os.path.join(ROOT_DIR,INPUT_FOLDER_NAME)\n",
    "PUNCTUATED_FOLDER = os.path.join(ROOT_DIR,PUNCTUATED_FOLDER_NAME)\n",
    "\n",
    "if not os.path.exists(INPUT_FOLDER):\n",
    "    os.makedirs(INPUT_FOLDER)\n",
    "    \n",
    "if not os.path.exists(PUNCTUATED_FOLDER):\n",
    "    os.makedirs(PUNCTUATED_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from Firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIREBASE_FOLDER_NAME = \"FIREBASE_STORAGE_RECORDS\"\n",
    "\n",
    "FIREBASE_FOLDER =os.path.join(ROOT_DIR,FIREBASE_FOLDER_NAME)\n",
    "\n",
    "if not os.path.exists(FIREBASE_FOLDER):\n",
    "    os.makedirs(FIREBASE_FOLDER)\n",
    "    \n",
    "    \n",
    "# cred = credentials.Certificate(\"FIREBASE_STORAGE_RECORDS/studentplanner-1712e-firebase-adminsdk-j4w7q-b1ff384c50.json\")\n",
    "# firebase_admin.initialize_app(cred)\n",
    "    \n",
    "config= {\n",
    "  \"apiKey\": \"AIzaSyA-neHuZcqrBlCFXVqk7kppz9FmRivxtMY\",\n",
    "  \"authDomain\": \"studentplanner-1712e.firebaseapp.com\",\n",
    "  \"databaseURL\": \"https://studentplanner-1712e.firebaseio.com\",\n",
    "  \"projectId\": \"studentplanner-1712e\",\n",
    "  \"storageBucket\": \"studentplanner-1712e.appspot.com\",\n",
    "  \"messagingSenderId\": \"447714992182\",\n",
    "  \"appId\": \"1:447714992182:web:876ddd230913e1482b7897\",\n",
    "  \"serviceAccount\": \"FIREBASE_STORAGE_RECORDS/studentplanner-1712e-firebase-adminsdk-j4w7q-b1ff384c50.json\"\n",
    "}\n",
    "\n",
    "firebase = pyrebase.initialize_app(config)\n",
    "storage = firebase.storage()\n",
    "\n",
    "\n",
    "datadir = 'Audio_Recorder/'\n",
    "\n",
    "all_files = storage.child(\"Audio_Recorder\").list_files()\n",
    "\n",
    "\n",
    "\n",
    "for file in all_files:\n",
    "    \n",
    "\n",
    "    file_str = str(file)\n",
    "    file_list = file_str.split(\",\")\n",
    "    wanted_file = file_list[1].strip() \n",
    "    real_wanted_file = wanted_file.split('/')[0].strip()  \n",
    "    \n",
    "    if not real_wanted_file == \"Audio_Recorder\" :\n",
    "        \n",
    "        print(\"STOP RUNNING PROGRAMME\")\n",
    "        break\n",
    "            \n",
    "    else:\n",
    "\n",
    "        real_wanted_file_name = wanted_file.split('/')[1].strip()\n",
    "\n",
    "        print('real_wanted_file_name = {}'.format(real_wanted_file_name))\n",
    "\n",
    "        firebase = pyrebase.initialize_app(config)\n",
    "        storage = firebase.storage()\n",
    "\n",
    "\n",
    "        path_on_cloud = 'Audio_Recorder/{}'.format(real_wanted_file_name)\n",
    "        path_local = 'FIREBASE_STORAGE_RECORDS/{}.wav'.format(real_wanted_file_name)\n",
    "\n",
    "        print('path_on_cloud = {}'.format(path_on_cloud))\n",
    "        print('path_local = {}'.format(path_local))\n",
    "\n",
    "\n",
    "        storage.child(path_on_cloud).download(path_local)\n",
    "        \n",
    "        print(\"AUDIO FILE DOWNLOADED!\")\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech to Text Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AUDIO_FILE = path_local\n",
    "# AUDIO_FILE = (\"speech2_FEMALE.wav\")\n",
    "# AUDIO_FILE = (\"speech3_INDIAN.wav\")\n",
    "\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "with sr.AudioFile(AUDIO_FILE) as source:  \n",
    "    audio = r.record(source)   \n",
    "\n",
    "total_text = r.recognize_google(audio)\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "print(type(total_text))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "with open(\"{}\\SAMPLE_1.txt\".format(INPUT_FOLDER_NAME), \"w\") as text_file:\n",
    "    text_file.write(total_text)\n",
    "    \n",
    "    \n",
    "\n",
    "!cat INPUT\\SAMPLE_1.txt | python punctuator.py Demo-Europarl-EN.pcl OUTPUT\\OUTPUT_1.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "\n",
    "\n",
    "def plural_to_singular(keyword):\n",
    "    \n",
    "    text_blob_object = TextBlob(keyword)\n",
    "    \n",
    "    text_list = text_blob_object.words.singularize()\n",
    "    \n",
    "    new_keyword = listToString(text_list)\n",
    "    \n",
    "    return new_keyword\n",
    "\n",
    "# DICTIONARY INPUT fUNCTION\n",
    "        \n",
    "def final_dict_function(dictionary_name, input_lvl_1_word, input_lvl_2_word, INFORMATION):\n",
    "    \n",
    "    for level_1_keyword in dictionary_name.copy():\n",
    "\n",
    "        if level_1_keyword == input_lvl_1_word:\n",
    "    \n",
    "            for level_2_keyword in dictionary_name[level_1_keyword]:\n",
    "            \n",
    "                if level_2_keyword == input_lvl_2_word:\n",
    "                    dictionary_name[level_1_keyword][level_2_keyword] = INFORMATION\n",
    "\n",
    "\n",
    "    return dictionary_name\n",
    "\n",
    "\n",
    "\n",
    "def find_synonym_list(word):\n",
    "    \n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            \n",
    "    return list(set(synonyms))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_keywords_level_1_ori = ['assignment','coursework', 'homework','tutorial', 'lab','exam','test']\n",
    "important_keywords_level_2_ori = ['title', 'objective','objectives','deadline', 'submission',\\\n",
    "                             'requirement']\n",
    "\n",
    "target_word_list_1 = []\n",
    "target_word_list_2 = []\n",
    "target_word_dict_level_1 = {}\n",
    "wanted_word_L1_list = []\n",
    "wanted_word_L2_list = []\n",
    "essence_list = []\n",
    "final_essence_list = []\n",
    "\n",
    "important_keywords_level_1 = []\n",
    "important_keywords_level_2 = []\n",
    "\n",
    "\n",
    "essence_list = []\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "with open(\"{}\\OUTPUT_1.txt\".format(PUNCTUATED_FOLDER_NAME), \"r\") as read_file:\n",
    "    punctuated_text = read_file.read()\n",
    "#     print(punctuated_text)\n",
    "    \n",
    "    \n",
    "new_punctuated_text = punctuated_text.replace('COMMA','').replace('PERIOD','').replace('COLON','')\n",
    "new_punctuated_text_list = sent_tokenize(new_punctuated_text)\n",
    "# print(new_punctuated_text_list)\n",
    "\n",
    "\n",
    "total_word_list = word_tokenize(new_punctuated_text)\n",
    "\n",
    "\n",
    "for word in important_keywords_level_1_ori:\n",
    "    \n",
    "    \n",
    "    \n",
    "    similar_word_list = find_synonym_list(word)\n",
    "    \n",
    "    important_keywords_level_1.append(word)\n",
    "    important_keywords_level_1.extend(similar_word_list)\n",
    "\n",
    "    \n",
    "for word in important_keywords_level_2_ori:\n",
    "    \n",
    "    \n",
    "    \n",
    "    similar_word_list = find_synonym_list(word)\n",
    "    \n",
    "    important_keywords_level_2.append(word)\n",
    "    important_keywords_level_2.extend(similar_word_list)\n",
    "    \n",
    "    \n",
    "print(important_keywords_level_1)\n",
    "\n",
    "print('\\n')\n",
    "print(important_keywords_level_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for word in total_word_list:\n",
    "    if word in important_keywords_level_1:\n",
    "        \n",
    "       \n",
    "        \n",
    "        target_word_list_1.append(word)\n",
    "        \n",
    "    if word in important_keywords_level_2:\n",
    "        \n",
    "        \n",
    "        \n",
    "        target_word_list_2.append(word)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for sentence in new_punctuated_text_list:\n",
    "\n",
    "    \n",
    "    sentence_list = word_tokenize(sentence)\n",
    "    remove_word_list =[]\n",
    "    \n",
    "    essence_list = []\n",
    "    \n",
    "    for word in sentence_list:\n",
    "        \n",
    "        \n",
    "        \n",
    "        if word in target_word_list_1:\n",
    "            print('word = {}'.format(word))\n",
    "            \n",
    "            remove_word_list.append(word)\n",
    "            \n",
    "            wanted_word_L1_list.append(word)\n",
    "            \n",
    "              \n",
    "        if word in target_word_list_2:\n",
    "            \n",
    "            \n",
    "            \n",
    "            remove_word_list.append(word)\n",
    "            wanted_word_L2_list.append(word)\n",
    "\n",
    "    TARGET_1 = any(elem in important_keywords_level_1 for elem in remove_word_list)\n",
    "    TARGET_2 = any(elem in important_keywords_level_2 for elem in remove_word_list)\n",
    "    \n",
    "    CONDITION = TARGET_1 or TARGET_2\n",
    "    \n",
    "    if CONDITION == True:\n",
    "        \n",
    "        for index , remove_word in enumerate(remove_word_list):\n",
    "            \n",
    "            if index == 0: \n",
    "                print('remove_word = {}'.format(remove_word))\n",
    "                important_information = sentence.replace(remove_word,'')\n",
    "                 \n",
    "            else:\n",
    "                print('remove_word = {}'.format(remove_word))\n",
    "                important_information = important_information.replace(remove_word,'')\n",
    "\n",
    "        \n",
    "        tokens = word_tokenize(important_information)\n",
    "        result = [i for i in tokens if not i in stop_words]\n",
    "        seperator = ' '\n",
    "        new_important_information = seperator.join(result)\n",
    "        \n",
    "\n",
    "        essence_list.append(remove_word_list)\n",
    "        essence_list.append(new_important_information)\n",
    "        \n",
    "        \n",
    "        final_essence_list.append(essence_list)\n",
    "       \n",
    "                \n",
    "        \n",
    "        \n",
    "        print('remove_word_list ={}'.format(remove_word_list))\n",
    "        print('original sentence = {}'.format(sentence))\n",
    "        print('important_information ={}'.format(important_information))\n",
    "        print('new important_information ={}'.format(new_important_information))\n",
    "        print('\\n')\n",
    "#         print('new_essence_for_dict={}'.format(new_essence_for_dict))\n",
    "        print('essence list = {}'.format(essence_list))\n",
    "        \n",
    "        \n",
    "    print('\\n')\n",
    "    print('---------next sentence ------------')\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "print(final_essence_list)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "old_wanted_word_L2_list = wanted_word_L2_list\n",
    "        \n",
    "    \n",
    "    \n",
    "text_blob_object_1 = TextBlob(listToString(wanted_word_L1_list))\n",
    "text_blob_object_2 = TextBlob(listToString(wanted_word_L2_list))\n",
    "\n",
    "wanted_word_L1_list = text_blob_object_1.words.singularize()\n",
    "wanted_word_L2_list = text_blob_object_2.words.singularize()\n",
    "    \n",
    "    \n",
    "print('WANTED_WORD_L1_LIST = {}'.format(wanted_word_L1_list))\n",
    "print('WANTED_WORD_L2_LIST = {}'.format(wanted_word_L2_list))\n",
    "print('OLD_WANTED_WORD_L2_LIST = {}'.format(old_wanted_word_L2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wanted_word_L1_list = []\n",
    "\n",
    "print('wanted_word_L1_list = {}'.format(wanted_word_L1_list))\n",
    "print('\\n')\n",
    "\n",
    "for item in wanted_word_L1_list:\n",
    "    \n",
    "    index = 1\n",
    "    print(\"current item is: {}\".format(item))\n",
    "    \n",
    "    if item in new_wanted_word_L1_list:\n",
    "        while item in new_wanted_word_L1_list:\n",
    "            \n",
    "            \n",
    "            index += 1\n",
    "            temp_item = item.split('_')\n",
    "            item = temp_item[0]\n",
    "            item = '{}_{}'.format(item, index)\n",
    "            \n",
    "\n",
    "        new_wanted_word_L1_list.append(item)\n",
    "    else:\n",
    "        new_wanted_word_L1_list.append(item)\n",
    "        \n",
    "        \n",
    "print('\\n')    \n",
    "\n",
    "print('new_wanted_word_L1_list = {}'.format(new_wanted_word_L1_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization of Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanted_word_L1_list = list(dict.fromkeys(wanted_word_L1_list))\n",
    "wanted_word_L2_list = list(dict.fromkeys(wanted_word_L2_list))\n",
    "\n",
    "print('wanted_word_L1_list: {}'.format(wanted_word_L1_list))\n",
    "print('wanted_word_L2_list: {}'.format(wanted_word_L2_list))\n",
    "\n",
    "wanted_word_L2_dict = dict.fromkeys(wanted_word_L2_list)\n",
    "print(wanted_word_L2_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------------------------')\n",
    "\n",
    "# Dictionary Creation \n",
    "for key in new_wanted_word_L1_list:\n",
    "    \n",
    "    \n",
    "    \n",
    "    if key in target_word_dict_level_1:\n",
    "\n",
    "        print('key = {}'.format(key))\n",
    "#         if key == 'assignment':\n",
    "\n",
    "#             target_word_dict_level_1[key]\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        target_word_dict_level_1[key]= wanted_word_L2_dict\n",
    "        \n",
    "print('\\n')\n",
    "\n",
    "target_word_dict_level_1_str = str(target_word_dict_level_1)\n",
    "final_dict_str = target_word_dict_level_1_str.replace('[','{').replace(']','}')\n",
    "\n",
    "# final_dict = json.loads(final_dict_str) \n",
    "final_dict = ast.literal_eval(final_dict_str) \n",
    "\n",
    "# print(final_dict)\n",
    "# print(type(final_dict))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "              \n",
    "print('---------------------------------------------------------------------------------------------------')\n",
    "\n",
    "temp_lvl_1_list = []\n",
    "\n",
    "# Input to Dictionary \n",
    "\n",
    "for item_level_1 in final_essence_list:\n",
    "#     print(item_level_1)\n",
    "\n",
    "    index = 1\n",
    "    \n",
    "    obtained_keyword = item_level_1[0]\n",
    "    INFORMATION = item_level_1[1]\n",
    "    \n",
    "\n",
    "    level_1_keyword_set = set(wanted_word_L1_list).intersection(obtained_keyword)\n",
    "    level_2_keyword_set = set(old_wanted_word_L2_list).intersection(obtained_keyword)\n",
    "    \n",
    "    level_1_keyword = repr(level_1_keyword_set)\n",
    "    level_2_keyword = repr(level_2_keyword_set)\n",
    "    \n",
    "    \n",
    "    \n",
    "    TARGET_3 = any(elem in wanted_word_L1_list for elem in obtained_keyword)\n",
    "    TARGET_4 = any(elem in old_wanted_word_L2_list for elem in obtained_keyword)\n",
    "    \n",
    "    \n",
    "    \n",
    "    index = 1\n",
    "    \n",
    "    \n",
    "    if TARGET_3 == False:\n",
    "        level_1_keyword_stripped = None\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        level_1_keyword_stripped = level_1_keyword[2:-2]\n",
    "        \n",
    "        level_1_keyword_stripped = plural_to_singular(level_1_keyword_stripped)\n",
    "        \n",
    "       \n",
    "        \n",
    "        if level_1_keyword_stripped in temp_lvl_1_list:\n",
    "            while level_1_keyword_stripped in temp_lvl_1_list:\n",
    "\n",
    "\n",
    "                index += 1\n",
    "                temp_item = level_1_keyword_stripped.split('_')\n",
    "                level_1_keyword_stripped = temp_item[0]\n",
    "                level_1_keyword_stripped = '{}_{}'.format(level_1_keyword_stripped, index)\n",
    "                \n",
    "                previous_lvl_1 = level_1_keyword_stripped\n",
    "\n",
    "\n",
    "            temp_lvl_1_list.append(level_1_keyword_stripped)\n",
    "\n",
    "        else:\n",
    "            temp_lvl_1_list.append(level_1_keyword_stripped)\n",
    "    \n",
    "            previous_lvl_1 = level_1_keyword_stripped\n",
    "    \n",
    "\n",
    "    \n",
    "    if TARGET_4 == False:\n",
    "        level_2_keyword_stripped = None\n",
    "        \n",
    "    else:\n",
    "        level_2_keyword_stripped = level_2_keyword[2:-2]\n",
    "        \n",
    "        level_2_keyword_stripped = plural_to_singular(level_2_keyword_stripped)\n",
    "        \n",
    "    \n",
    "    print('level_1_keyword_ = {}'.format(level_1_keyword))\n",
    "    print('level_2_keyword_ = {}'.format(level_2_keyword))\n",
    "    \n",
    "    \n",
    "    print('level_1_keyword_stripped = {}'.format(level_1_keyword_stripped))\n",
    "    print('level_2_keyword_stripped = {}'.format(level_2_keyword_stripped))\n",
    "    print('INFORMATION = {}'.format(INFORMATION))\n",
    "    \n",
    "\n",
    "    if level_1_keyword_stripped == None:\n",
    "        \n",
    "        final_dict_new = final_dict_function(final_dict, previous_lvl_1, level_2_keyword_stripped, INFORMATION)\n",
    "\n",
    "    \n",
    "    else:  \n",
    "    \n",
    "        final_dict_new = final_dict_function(final_dict, level_1_keyword_stripped, level_2_keyword_stripped, INFORMATION)\n",
    "        \n",
    "        \n",
    "    \n",
    "    print('\\n')\n",
    "    print('----------next item--------------')\n",
    "    print('\\n')\n",
    "    \n",
    "            \n",
    "        \n",
    "\n",
    "print(final_dict_new)\n",
    "\n",
    "    \n",
    "# print(temp_lvl_1_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_format_dataframe = pd.DataFrame.from_dict(final_dict_new)\n",
    "\n",
    "# print(new_format_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output for usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections \n",
    "\n",
    "JSON_FILE_NAME = 'TESTING.json'\n",
    "\n",
    "od = collections.OrderedDict(sorted(final_dict_new.items()))\n",
    "\n",
    "with open(JSON_FILE_NAME,'w') as g:\n",
    "    json.dump(od,g)\n",
    "    \n",
    "    \n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload processed data into Firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firebase import firebase\n",
    "\n",
    "firebase = firebase.FirebaseApplication('https://studentplanner-1712e.firebaseio.com', None)\n",
    "new_user = 'Lecture_{}'.format(today)\n",
    "\n",
    "result = firebase.put('/Assignment',new_user, final_dict_new)\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove audio file from local/Firebase storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_local):\n",
    "    os.remove(path_local)   #remove local storage\n",
    "\n",
    "    print(\"Firebase Audio File Deleted!\")\n",
    "\n",
    "\n",
    "storage.delete(path_on_cloud)     #remove Firebase storage \n",
    "\n",
    "print(\"Firebase Audio File Deleted!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
